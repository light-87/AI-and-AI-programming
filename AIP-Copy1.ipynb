{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b0dc2ae-91cf-4a66-9bd8-ba01ffdec929",
   "metadata": {},
   "source": [
    "# ***0. Data Loading***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321e17c1-6c87-4777-b420-26f3f1919081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"UNSWNB15_training_coursework.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff593e92-2dc8-49fe-9bd7-4232163d31e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31510d30-d4dc-4847-8879-ef1e06ddf209",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28887f56-25a2-4065-ae2d-986de22f164d",
   "metadata": {},
   "source": [
    "# ***1. Data Pre-Processing (Task 1)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e32a2fe-3efb-42ac-9d9a-2e0724551d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c80d4d-293e-4392-8ab5-9f19a8064353",
   "metadata": {},
   "outputs": [],
   "source": [
    "categori_col = []\n",
    "for i in df.columns:\n",
    "    if df[i].nunique() < 25:\n",
    "        categori_col.append(i)\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b7cbb8-8400-49c1-aab4-6bf8bd50b133",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in categori_col:\n",
    "    print(f\"Column: {i}\")\n",
    "    print(df[i].value_counts())\n",
    "    print(\"=====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cef879-47f8-497c-8fe0-c45911a1f33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"proto\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe6c3cb-e50c-4646-bd73-16063d1b8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "categorical_cols = ['service', 'state']\n",
    "\n",
    "def one_hot_encode(df, categorical_cols):\n",
    "    df_encoded = df.copy()\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        dummies = pd.get_dummies(df_encoded[col], prefix=col, drop_first=False)\n",
    "        df_encoded = pd.concat([df_encoded, dummies], axis=1)\n",
    "        df_encoded.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "df_encoded = one_hot_encode(df, categorical_cols)\n",
    "df_encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8c9902-87bb-4ab0-92b9-4d77a6a86e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Identify the most common protocols and group the rest\n",
    "def encode_protocol(df, column='proto', top_n=10):\n",
    "    # Get value counts\n",
    "    value_counts = df[column].value_counts()\n",
    "    \n",
    "    # Identify top N values\n",
    "    top_values = value_counts.head(top_n).index.tolist()\n",
    "    \n",
    "    # Create a copy of the dataframe\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    # Replace all non-top values with 'other'\n",
    "    df_encoded[column] = df_encoded[column].apply(lambda x: x if x in top_values else 'other')\n",
    "    \n",
    "    # One-hot encode the modified column\n",
    "    proto_encoded = pd.get_dummies(df_encoded[column], prefix=column, drop_first=False)\n",
    "    \n",
    "    # Concatenate with original dataframe\n",
    "    df_encoded = pd.concat([df_encoded, proto_encoded], axis=1)\n",
    "    \n",
    "    # Drop the original column\n",
    "    df_encoded.drop(column, axis=1, inplace=True)\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "# Apply the protocol encoding\n",
    "df_encoded = encode_protocol(df_encoded, top_n=10)\n",
    "\n",
    "# Check the new dummy columns\n",
    "proto_columns = [col for col in df_encoded.columns if col.startswith('proto_')]\n",
    "print(f\"Protocol encoded columns: {proto_columns}\")\n",
    "print(f\"Shape after encoding proto: {df_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e21a67-dcaf-46b3-891b-741924ee2087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_feature_correlations(df, target='label'):\n",
    "    correlations = df.corr()[target].sort_values(ascending=False)\n",
    "    print(\"Top positive correlations with target:\")\n",
    "    print(correlations[correlations > 0].head(10))\n",
    "    print(\"\\nTop negative correlations with target:\")\n",
    "    print(correlations[correlations < 0].head(10))\n",
    "    \n",
    "    return correlations\n",
    "\n",
    "target_correlations = check_feature_correlations(df_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7506e2-3c99-43c3-a080-119924a5acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b159ea43-980a-496e-900b-65e9712987db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import time\n",
    "\n",
    "def standardize_features(df, exclude_cols=None):\n",
    "    \"\"\"\n",
    "    Standardize features to have zero mean and unit variance\n",
    "    exclude_cols: list of columns to exclude from standardization (e.g., target variable)\n",
    "    \"\"\"\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = []\n",
    "    \n",
    "    # Create a copy of the dataframe\n",
    "    df_std = df.copy()\n",
    "    \n",
    "    # Get columns to standardize\n",
    "    cols_to_standardize = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Calculate mean and std, and standardize\n",
    "    for col in cols_to_standardize:\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "        # Avoid division by zero\n",
    "        if std > 0:\n",
    "            df_std[col] = (df[col] - mean) / std\n",
    "        else:\n",
    "            df_std[col] = 0  # Set to zero if std is zero\n",
    "    \n",
    "    # Store the mean and std for later use (e.g., with test data)\n",
    "    scaler_params = {col: {'mean': df[col].mean(), 'std': df[col].std()} \n",
    "                     for col in cols_to_standardize}\n",
    "    \n",
    "    return df_std, scaler_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f88832-e45b-4ddf-a678-919d5dde732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validation_split(df, val_ratio=0.2, random_state=87):\n",
    "    \"\"\"\n",
    "    Split the dataset into training and validation sets\n",
    "    val_ratio: proportion of data to use for validation\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Get indices and shuffle\n",
    "    indices = np.arange(len(df))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Calculate split point\n",
    "    val_size = int(val_ratio * len(df))\n",
    "    \n",
    "    # Split indices\n",
    "    val_indices = indices[:val_size]\n",
    "    train_indices = indices[val_size:]\n",
    "    \n",
    "    # Create train and validation dataframes\n",
    "    train_df = df.iloc[train_indices].reset_index(drop=True)\n",
    "    val_df = df.iloc[val_indices].reset_index(drop=True)\n",
    "    \n",
    "    return train_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee112624-bf43-4ff8-86f6-f28cf5fd1f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBFN:\n",
    "    def __init__(self, num_centers, sigma=1.0):\n",
    "        \"\"\"\n",
    "        Initialize RBFN\n",
    "        num_centers: number of RBF neurons/centers\n",
    "        sigma: width parameter for RBF neurons\n",
    "        \"\"\"\n",
    "        self.num_centers = num_centers\n",
    "        self.sigma = sigma\n",
    "        self.centers = None\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def _kmeans(self, X, max_iters=200, tol=1e-4):\n",
    "        \"\"\"\n",
    "        Implement K-means algorithm for finding centers\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize centers by randomly selecting data points\n",
    "        random_indices = np.random.choice(n_samples, self.num_centers, replace=False)\n",
    "        centers = X[random_indices].copy()\n",
    "        \n",
    "        # Iteratively update centers\n",
    "        for iteration in range(max_iters):\n",
    "            # Assign each point to the nearest center\n",
    "            distances = np.zeros((n_samples, self.num_centers))\n",
    "            for i, center in enumerate(centers):\n",
    "                # Calculate squared Euclidean distance\n",
    "                diff = X - center\n",
    "                distances[:, i] = np.sum(diff * diff, axis=1)\n",
    "            \n",
    "            # Get cluster assignments\n",
    "            cluster_assignments = np.argmin(distances, axis=1)\n",
    "            \n",
    "            # Store old centers for convergence check\n",
    "            old_centers = centers.copy()\n",
    "            \n",
    "            # Update centers\n",
    "            for i in range(self.num_centers):\n",
    "                # Get points assigned to this cluster\n",
    "                cluster_points = X[cluster_assignments == i]\n",
    "                if len(cluster_points) > 0:\n",
    "                    centers[i] = np.mean(cluster_points, axis=0)\n",
    "            \n",
    "            # Check for convergence\n",
    "            center_shift = np.sum((centers - old_centers) ** 2)\n",
    "            if center_shift < tol:\n",
    "                break\n",
    "        \n",
    "        return centers\n",
    "    \n",
    "    def _calculate_sigma(self, centers):\n",
    "        \"\"\"\n",
    "        Calculate sigma based on average distance between centers\n",
    "        \"\"\"\n",
    "        # Calculate pairwise distances between centers\n",
    "        n_centers = centers.shape[0]\n",
    "        distances = np.zeros((n_centers, n_centers))\n",
    "        \n",
    "        for i in range(n_centers):\n",
    "            for j in range(i+1, n_centers):\n",
    "                dist = np.sqrt(np.sum((centers[i] - centers[j]) ** 2))\n",
    "                distances[i, j] = distances[j, i] = dist\n",
    "        \n",
    "        # Average distance to closest center\n",
    "        min_distances = []\n",
    "        for i in range(n_centers):\n",
    "            # Filter out self-distance (which is 0)\n",
    "            center_distances = [d for j, d in enumerate(distances[i]) if j != i and d > 0]\n",
    "            if center_distances:\n",
    "                min_distances.append(min(center_distances))\n",
    "        \n",
    "        # If can't calculate, use default value\n",
    "        if not min_distances:\n",
    "            return self.sigma\n",
    "        \n",
    "        # Set sigma as average minimum distance / sqrt(2*num_centers)\n",
    "        sigma = np.mean(min_distances) / math.sqrt(2 * self.num_centers)\n",
    "        return max(sigma, 1e-10)  # Avoid too small sigma\n",
    "    \n",
    "    def _rbf(self, x, center):\n",
    "        \"\"\"\n",
    "        Apply Gaussian RBF to a data point\n",
    "        \"\"\"\n",
    "        dist = np.sum((x - center) ** 2)\n",
    "        return np.exp(-dist / (2 * self.sigma ** 2))\n",
    "    \n",
    "    def _calculate_interpolation_matrix(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the interpolation matrix (RBF outputs)\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        G = np.zeros((n_samples, self.num_centers))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            for j in range(self.num_centers):\n",
    "                G[i, j] = self._rbf(X[i], self.centers[j])\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the RBFN\n",
    "        X: input features [n_samples, n_features]\n",
    "        y: target values [n_samples]\n",
    "        \"\"\"\n",
    "        # Convert to numpy arrays\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        # Find centers using K-means\n",
    "        print(\"Finding centers using K-means...\")\n",
    "        self.centers = self._kmeans(X)\n",
    "        \n",
    "        # Calculate optimal sigma\n",
    "        # self.sigma = self._calculate_sigma(self.centers)\n",
    "        # print(f\"Using sigma: {self.sigma}\")\n",
    "        \n",
    "        # Calculate interpolation matrix\n",
    "        print(\"Calculating interpolation matrix...\")\n",
    "        G = self._calculate_interpolation_matrix(X)\n",
    "        \n",
    "        # Add a column of ones for bias\n",
    "        G_with_bias = np.column_stack((G, np.ones(X.shape[0])))\n",
    "        \n",
    "        # Solve for weights using pseudoinverse (normal equation)\n",
    "        print(\"Solving for weights...\")\n",
    "        try:\n",
    "            # Using pseudoinverse: (G^T G)^(-1) G^T y\n",
    "            GTG = np.dot(G_with_bias.T, G_with_bias)\n",
    "            GTG_inv = np.linalg.inv(GTG + np.eye(GTG.shape[0]) * 1e-6)  # Add small regularization\n",
    "            self.weights = np.dot(np.dot(GTG_inv, G_with_bias.T), y)\n",
    "            \n",
    "            # Extract bias term\n",
    "            self.bias = self.weights[-1]\n",
    "            # Keep only RBF weights\n",
    "            self.weights = self.weights[:-1]\n",
    "            \n",
    "            # Calculate training error\n",
    "            y_pred = self.predict(X)\n",
    "            accuracy = np.mean(y_pred == y)\n",
    "            print(f\"Training accuracy: {accuracy:.4f}\")\n",
    "            \n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"Error: Matrix inversion failed. Try reducing the number of centers or adding more regularization.\")\n",
    "            # Set random weights as fallback\n",
    "            self.weights = np.random.randn(self.num_centers)\n",
    "            self.bias = 0\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions using the trained RBFN\n",
    "        X: input features [n_samples, n_features]\n",
    "        Returns: predicted classes [n_samples]\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        \n",
    "        # Calculate RBF outputs\n",
    "        G = self._calculate_interpolation_matrix(X)\n",
    "        \n",
    "        # Apply weights and add bias\n",
    "        y_pred_raw = np.dot(G, self.weights) + self.bias\n",
    "        \n",
    "        # Apply threshold for binary classification\n",
    "        y_pred = (y_pred_raw > 0.5).astype(int)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict probability estimates\n",
    "        X: input features [n_samples, n_features]\n",
    "        Returns: probabilities [n_samples]\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        \n",
    "        # Calculate RBF outputs\n",
    "        G = self._calculate_interpolation_matrix(X)\n",
    "        \n",
    "        # Apply weights and add bias\n",
    "        y_pred_raw = np.dot(G, self.weights) + self.bias\n",
    "        \n",
    "        # Apply sigmoid to get probabilities\n",
    "        y_proba = 1 / (1 + np.exp(-y_pred_raw))\n",
    "        \n",
    "        return y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17d8ab2-629e-4cd3-bb1c-96b4e8d7b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate performance metrics\n",
    "    \"\"\"\n",
    "    # Accuracy\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    \n",
    "    # Confusion matrix elements\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    # Precision, recall, F1\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Balanced accuracy\n",
    "    balanced_acc = ((tp / (tp + fn) if (tp + fn) > 0 else 0) + \n",
    "                    (tn / (tn + fp) if (tn + fp) > 0 else 0)) / 2\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "        'confusion_matrix': {\n",
    "            'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c6dd0-f04d-48c0-aa81-72d23375f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate performance metrics\n",
    "    \"\"\"\n",
    "    # Accuracy\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    \n",
    "    # Confusion matrix elements\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    # Precision, recall, F1\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Balanced accuracy\n",
    "    balanced_acc = ((tp / (tp + fn) if (tp + fn) > 0 else 0) + \n",
    "                    (tn / (tn + fp) if (tn + fp) > 0 else 0)) / 2\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "        'confusion_matrix': {\n",
    "            'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a957e483-1351-4432-a884-d846f892d8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate performance metrics\n",
    "    \"\"\"\n",
    "    # Accuracy\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    \n",
    "    # Confusion matrix elements\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    # Precision, recall, F1\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Balanced accuracy\n",
    "    balanced_acc = ((tp / (tp + fn) if (tp + fn) > 0 else 0) + \n",
    "                    (tn / (tn + fp) if (tn + fp) > 0 else 0)) / 2\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "        'confusion_matrix': {\n",
    "            'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb474a94-743e-4bc1-9749-2d9b2cfbeae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_rbfn(X_train, y_train, X_val, y_val, param_grid):\n",
    "    \"\"\"\n",
    "    Perform grid search to find optimal hyperparameters for RBFN\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, y_train: Training data and labels\n",
    "    X_val, y_val: Validation data and labels\n",
    "    param_grid: Dictionary with hyperparameters to search\n",
    "                Example: {'num_centers': [10, 30, 50], 'sigma': [0.1, 1.0, 5.0]}\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    best_params: Dictionary with best parameters\n",
    "    best_model: Trained model with best parameters\n",
    "    results: Dictionary with all results for analysis\n",
    "    \"\"\"\n",
    "    print(\"\\nStarting grid search for RBFN hyperparameters...\")\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    best_accuracy = 0.0\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    results = []\n",
    "    \n",
    "    # Get all parameter combinations\n",
    "    param_combinations = []\n",
    "    \n",
    "    # Function to recursively build parameter combinations\n",
    "    def build_param_combinations(current_combo, param_names, param_values, index):\n",
    "        if index == len(param_names):\n",
    "            param_combinations.append(current_combo.copy())\n",
    "            return\n",
    "        \n",
    "        param_name = param_names[index]\n",
    "        for value in param_values[param_name]:\n",
    "            current_combo[param_name] = value\n",
    "            build_param_combinations(current_combo, param_names, param_values, index + 1)\n",
    "    \n",
    "    # Build all parameter combinations\n",
    "    param_names = list(param_grid.keys())\n",
    "    build_param_combinations({}, param_names, param_grid, 0)\n",
    "    \n",
    "    total_combinations = len(param_combinations)\n",
    "    print(f\"Testing {total_combinations} parameter combinations\")\n",
    "    \n",
    "    # Loop through all parameter combinations\n",
    "    for i, params in enumerate(param_combinations):\n",
    "        print(f\"\\nTesting combination {i+1}/{total_combinations}:\")\n",
    "        print(f\"Parameters: {params}\")\n",
    "        \n",
    "        # Create and train model with current parameters\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Initialize model with current parameters\n",
    "            model = RBFN(num_centers=params['num_centers'], sigma=params.get('sigma', 1.0))\n",
    "            \n",
    "            # Train the model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            y_val_pred = model.predict(X_val)\n",
    "            val_metrics = evaluate_model(y_val, y_val_pred)\n",
    "            accuracy = val_metrics['accuracy']\n",
    "            \n",
    "            # Record results\n",
    "            train_time = time.time() - start_time\n",
    "            result = {\n",
    "                'params': params.copy(),\n",
    "                'accuracy': accuracy,\n",
    "                'precision': val_metrics['precision'],\n",
    "                'recall': val_metrics['recall'],\n",
    "                'f1_score': val_metrics['f1_score'],\n",
    "                'balanced_accuracy': val_metrics['balanced_accuracy'],\n",
    "                'training_time': train_time\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"Accuracy: {accuracy:.4f}, Training time: {train_time:.2f}s\")\n",
    "            \n",
    "            # Update best if improved\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_params = params.copy()\n",
    "                best_model = model\n",
    "                print(f\"New best model found! Accuracy: {best_accuracy:.4f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with parameters {params}: {str(e)}\")\n",
    "            results.append({\n",
    "                'params': params.copy(),\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    # Sort results by accuracy\n",
    "    valid_results = [r for r in results if 'accuracy' in r]\n",
    "    sorted_results = sorted(valid_results, key=lambda x: x['accuracy'], reverse=True)\n",
    "    \n",
    "    print(\"\\nGrid Search Results:\")\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    print(f\"Best validation accuracy: {best_accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\nTop 5 parameter combinations:\")\n",
    "    for i, result in enumerate(sorted_results[:5]):\n",
    "        print(f\"{i+1}. {result['params']} - Accuracy: {result['accuracy']:.4f}\")\n",
    "    \n",
    "    return best_params, best_model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de78fc4-96a1-4283-a4e0-00b5e410f251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_best_model(best_model, best_params, model_params, X_cols):\n",
    "    \"\"\"\n",
    "    Test the best model from grid search on the test set 1\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    best_model: Trained RBFN model with best parameters\n",
    "    best_params: Dictionary of best parameters\n",
    "    model_params: Dictionary with scaler parameters\n",
    "    X_cols: List of feature columns used for training\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    test_metrics: Dictionary with test metrics\n",
    "    \"\"\"\n",
    "    print(\"\\nTesting best model on test set 1...\")\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    \n",
    "    try:\n",
    "        # Load test data\n",
    "        test_data = pd.read_csv('UNSWNB15_testing1_coursework.csv')\n",
    "        print(f\"Loaded test set with {test_data.shape[0]} samples\")\n",
    "        \n",
    "        # Perform one-hot encoding for categorical variables\n",
    "        categorical_cols = ['service', 'state']\n",
    "        test_encoded = one_hot_encode(test_data, categorical_cols)\n",
    "        \n",
    "        # Encode protocol column\n",
    "        test_encoded = encode_protocol(test_encoded, column='proto', top_n=10)\n",
    "        \n",
    "        # Handle missing columns from training data\n",
    "        for col in X_cols:\n",
    "            if col not in test_encoded.columns and col != 'label':\n",
    "                print(f\"Adding missing column: {col}\")\n",
    "                test_encoded[col] = 0\n",
    "        \n",
    "        # Handle extra columns in test data\n",
    "        extra_cols = [col for col in test_encoded.columns if col not in X_cols and col != 'label']\n",
    "        if extra_cols:\n",
    "            print(f\"Dropping extra columns: {extra_cols}\")\n",
    "            test_encoded.drop(columns=extra_cols, inplace=True)\n",
    "        \n",
    "        # Ensure all X_cols are present\n",
    "        missing_cols = [col for col in X_cols if col not in test_encoded.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing columns in test data: {missing_cols}\")\n",
    "        \n",
    "        # Standardize features using training parameters\n",
    "        test_std = pd.DataFrame()\n",
    "        scaler_params = model_params['scaler_params']\n",
    "        \n",
    "        for col in X_cols:\n",
    "            if col in scaler_params:\n",
    "                mean = scaler_params[col]['mean']\n",
    "                std = scaler_params[col]['std']\n",
    "                if std > 0:\n",
    "                    test_std[col] = (test_encoded[col] - mean) / std\n",
    "                else:\n",
    "                    test_std[col] = 0\n",
    "            else:\n",
    "                test_std[col] = test_encoded[col]\n",
    "        \n",
    "        # Extract features and target\n",
    "        X_test = test_std[X_cols].values\n",
    "        y_test = test_data['label'].values\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        \n",
    "        # Evaluate\n",
    "        test_metrics = evaluate_model(y_test, y_pred)\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\nTest Set Results:\")\n",
    "        print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
    "        print(f\"F1 Score: {test_metrics['f1_score']:.4f}\")\n",
    "        print(f\"Balanced Accuracy: {test_metrics['balanced_accuracy']:.4f}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(f\"TP: {test_metrics['confusion_matrix']['tp']}, TN: {test_metrics['confusion_matrix']['tn']}\")\n",
    "        print(f\"FP: {test_metrics['confusion_matrix']['fp']}, FN: {test_metrics['confusion_matrix']['fn']}\")\n",
    "        \n",
    "        # Calculate threshold requirements\n",
    "        required_accuracy = 0.85\n",
    "        achieved = test_metrics['accuracy'] >= required_accuracy\n",
    "        print(f\"\\nRequired accuracy: {required_accuracy:.2f}\")\n",
    "        print(f\"Achieved accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Requirement met: {achieved}\")\n",
    "        \n",
    "        # Bonus: Try to find optimal threshold for classification\n",
    "        if hasattr(best_model, 'predict_proba'):\n",
    "            print(\"\\nOptimizing decision threshold...\")\n",
    "            probs = best_model.predict_proba(X_test)\n",
    "            \n",
    "            # Test different thresholds\n",
    "            thresholds = np.linspace(0.1, 0.9, 9)\n",
    "            threshold_results = []\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                y_pred_threshold = (probs > threshold).astype(int)\n",
    "                metrics = evaluate_model(y_test, y_pred_threshold)\n",
    "                threshold_results.append({\n",
    "                    'threshold': threshold,\n",
    "                    'accuracy': metrics['accuracy'],\n",
    "                    'balanced_accuracy': metrics['balanced_accuracy'],\n",
    "                    'f1_score': metrics['f1_score']\n",
    "                })\n",
    "            \n",
    "            # Find best threshold\n",
    "            best_threshold = max(threshold_results, key=lambda x: x['accuracy'])\n",
    "            print(f\"Best threshold: {best_threshold['threshold']:.2f} with accuracy: {best_threshold['accuracy']:.4f}\")\n",
    "            \n",
    "            # Apply best threshold\n",
    "            y_pred_best = (probs > best_threshold['threshold']).astype(int)\n",
    "            optimized_metrics = evaluate_model(y_test, y_pred_best)\n",
    "            \n",
    "            print(\"\\nResults with optimized threshold:\")\n",
    "            print(f\"Accuracy: {optimized_metrics['accuracy']:.4f}\")\n",
    "            print(f\"Precision: {optimized_metrics['precision']:.4f}\")\n",
    "            print(f\"Recall: {optimized_metrics['recall']:.4f}\")\n",
    "            print(f\"F1 Score: {optimized_metrics['f1_score']:.4f}\")\n",
    "            print(f\"Balanced Accuracy: {optimized_metrics['balanced_accuracy']:.4f}\")\n",
    "        \n",
    "        return test_metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing model: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048c5513-93ac-4be7-be85-62f611ce14d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_set2(best_model, model_params, X_cols):\n",
    "    \"\"\"\n",
    "    Generate predictions for test set 2 (no labels)\n",
    "    \"\"\"\n",
    "    print(\"\\nPredicting on test set 2...\")\n",
    "    \n",
    "    try:\n",
    "        # Load test data\n",
    "        test_data = pd.read_csv('UNSWNB15_testing2_coursework_no_label.csv')\n",
    "        print(f\"Loaded test set 2 with {test_data.shape[0]} samples\")\n",
    "        \n",
    "        # Perform one-hot encoding for categorical variables\n",
    "        categorical_cols = ['service', 'state']\n",
    "        test_encoded = one_hot_encode(test_data, categorical_cols)\n",
    "        \n",
    "        # Encode protocol column\n",
    "        test_encoded = encode_protocol(test_encoded, column='proto', top_n=10)\n",
    "        \n",
    "        # Handle missing and extra columns\n",
    "        for col in X_cols:\n",
    "            if col not in test_encoded.columns:\n",
    "                print(f\"Adding missing column: {col}\")\n",
    "                test_encoded[col] = 0\n",
    "                \n",
    "        # Keep only the necessary columns\n",
    "        test_columns = [col for col in test_encoded.columns if col in X_cols]\n",
    "        test_encoded = test_encoded[test_columns]\n",
    "        \n",
    "        # Check if all features are present\n",
    "        missing_cols = [col for col in X_cols if col not in test_encoded.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing columns in test data: {missing_cols}\")\n",
    "        \n",
    "        # Standardize features\n",
    "        test_std = pd.DataFrame()\n",
    "        scaler_params = model_params['scaler_params']\n",
    "        \n",
    "        for col in X_cols:\n",
    "            if col in scaler_params:\n",
    "                mean = scaler_params[col]['mean']\n",
    "                std = scaler_params[col]['std']\n",
    "                if std > 0:\n",
    "                    test_std[col] = (test_encoded[col] - mean) / std\n",
    "                else:\n",
    "                    test_std[col] = 0\n",
    "            else:\n",
    "                test_std[col] = test_encoded[col]\n",
    "        \n",
    "        # Extract features\n",
    "        X_test = test_std[X_cols].values\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        \n",
    "        # Print predictions\n",
    "        print(\"\\nPredictions for test set 2:\")\n",
    "        print(y_pred)\n",
    "        \n",
    "        # Create results dataframe\n",
    "        results_df = pd.DataFrame({\n",
    "            'Index': range(len(y_pred)),\n",
    "            'Predicted_Label': y_pred\n",
    "        })\n",
    "        \n",
    "        # Save to CSV\n",
    "        results_path = 'test_set2_predictions.csv'\n",
    "        results_df.to_csv(results_path, index=False)\n",
    "        print(f\"Saved predictions to {results_path}\")\n",
    "        \n",
    "        return y_pred\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting on test set 2: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c6a394-faac-4f63-a425-bd8aef0e6fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(center):\n",
    "    # Load training data (adjust path as needed)\n",
    "    print(\"Loading training data...\")\n",
    "    train_data = df_encoded.copy()\n",
    "\n",
    "    print(\"\\nPerforming feature selection based on correlations...\")\n",
    "    # Select features with absolute correlation above a threshold\n",
    "    correlation_threshold = 0.1  # Adjust this threshold as needed\n",
    "    important_features = target_correlations.abs().sort_values(ascending=False)\n",
    "    selected_features = important_features[important_features > correlation_threshold].index.tolist()\n",
    "    \n",
    "    # Remove 'label' from selected features if present\n",
    "    if 'label' in selected_features:\n",
    "        selected_features.remove('label')\n",
    "    \n",
    "    print(f\"Selected {len(selected_features)} features with correlation > {correlation_threshold}\")\n",
    "    print(\"Top selected features:\")\n",
    "    print(selected_features[:10])  # Print top 10 features\n",
    "    \n",
    "    # Update X_cols to only use selected features\n",
    "    X_cols = selected_features\n",
    "\n",
    "    \n",
    "    # And 'label' is the target column\n",
    "    # X_cols = [col for col in train_data.columns if col != 'label']\n",
    "    y_col = 'label'\n",
    "    \n",
    "    # Standardize features\n",
    "    print(\"Standardizing features...\")\n",
    "    train_data_std, scaler_params = standardize_features(train_data, exclude_cols=[y_col])\n",
    "    \n",
    "    # Split into train and validation\n",
    "    print(\"Splitting data...\")\n",
    "    train_df, val_df = train_validation_split(train_data_std, val_ratio=0.2)\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train = train_df[X_cols].values\n",
    "    y_train = train_df[y_col].values\n",
    "    X_val = val_df[X_cols].values\n",
    "    y_val = val_df[y_col].values\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "    print(f\"Validation set: {X_val.shape[0]} samples, {X_val.shape[1]} features\")\n",
    "    \n",
    "    # # Define parameter grid for search\n",
    "    # param_grid = {\n",
    "    #     'num_centers': [30, 50, 70, 100],\n",
    "    #     'sigma': [0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "    # }\n",
    "    \n",
    "    # Define parameter grid for search\n",
    "    param_grid = {\n",
    "        'num_centers': [70, 100, 120,140],\n",
    "        'sigma': [2.0, 5.0, 7.0, 8.0, 9.0]\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Perform grid search\n",
    "    best_params, best_model, grid_results = grid_search_rbfn(X_train, y_train, X_val, y_val, param_grid)\n",
    "    \n",
    "    # Use the best model for final evaluation\n",
    "    rbfn = best_model\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "    \n",
    "    # # Train RBFN model\n",
    "    # print(\"Training RBFN model...\")\n",
    "    # start_time = time.time()\n",
    "    # rbfn = RBFN(num_centers=center, sigma=1.0)  # Adjust num_centers as needed\n",
    "    # rbfn.fit(X_train, y_train)\n",
    "    # training_time = time.time() - start_time\n",
    "    # print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(\"Evaluating on validation set...\")\n",
    "    y_val_pred = rbfn.predict(X_val)\n",
    "    val_metrics = evaluate_model(y_val, y_val_pred)\n",
    "    \n",
    "    print(\"\\nValidation Results:\")\n",
    "    print(f\"Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {val_metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {val_metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {val_metrics['f1_score']:.4f}\")\n",
    "    print(f\"Balanced Accuracy: {val_metrics['balanced_accuracy']:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(f\"TP: {val_metrics['confusion_matrix']['tp']}, TN: {val_metrics['confusion_matrix']['tn']}\")\n",
    "    print(f\"FP: {val_metrics['confusion_matrix']['fp']}, FN: {val_metrics['confusion_matrix']['fn']}\")\n",
    "    \n",
    "    # Save the model parameters (centers, weights, sigma) for later use\n",
    "    model_params = {\n",
    "        'centers': rbfn.centers,\n",
    "        'weights': rbfn.weights,\n",
    "        'bias': rbfn.bias,\n",
    "        'sigma': rbfn.sigma,\n",
    "        'scaler_params': scaler_params\n",
    "    }\n",
    "\n",
    "    test_metrics = test_best_model(best_model, best_params, model_params, X_cols)\n",
    "    test2_predictions = predict_test_set2(best_model, model_params, X_cols)\n",
    "    \n",
    "    return rbfn, model_params, val_metrics, test_metrics, test2_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc38dc9-63f5-458a-aaf8-55beb3b360c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rbfn, model_params, val_metrics, test_metrics, test2_predictions = main(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c2fc73-4ebe-441c-aad4-0b9c91657b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rbfn, model_params, val_metrics, test_metrics, test2_predictions = main(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5654dcd6-17cb-4e9d-9b6c-ac4333882f22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4330e859-144a-46b4-ac7e-e32bcc0cb160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Encoding functions\n",
    "def one_hot_encode(df, categorical_cols):\n",
    "    \"\"\"\n",
    "    One-hot encode categorical columns\n",
    "    \"\"\"\n",
    "    df_encoded = df.copy()\n",
    "    for col in categorical_cols:\n",
    "        if col in df_encoded.columns:\n",
    "            dummies = pd.get_dummies(df_encoded[col], prefix=col, drop_first=False)\n",
    "            df_encoded = pd.concat([df_encoded, dummies], axis=1)\n",
    "            df_encoded.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "def encode_protocol(df, column='proto', top_n=10):\n",
    "    \"\"\"\n",
    "    Encode protocol column by keeping top N values and grouping others\n",
    "    \"\"\"\n",
    "    if column not in df.columns:\n",
    "        return df\n",
    "        \n",
    "    # Get value counts\n",
    "    value_counts = df[column].value_counts()\n",
    "    \n",
    "    # Identify top N values\n",
    "    top_values = value_counts.head(top_n).index.tolist()\n",
    "    \n",
    "    # Create a copy of the dataframe\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    # Replace all non-top values with 'other'\n",
    "    df_encoded[column] = df_encoded[column].apply(lambda x: x if x in top_values else 'other')\n",
    "    \n",
    "    # One-hot encode the modified column\n",
    "    proto_encoded = pd.get_dummies(df_encoded[column], prefix=column, drop_first=False)\n",
    "    \n",
    "    # Concatenate with original dataframe\n",
    "    df_encoded = pd.concat([df_encoded, proto_encoded], axis=1)\n",
    "    \n",
    "    # Drop the original column\n",
    "    df_encoded.drop(column, axis=1, inplace=True)\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "# 5. Main execution\n",
    "def main():\n",
    "    # Load training data\n",
    "    print(\"Loading training data...\")\n",
    "    train_data = pd.read_csv('UNSWNB15_training_coursework.csv')\n",
    "    \n",
    "    # Display basic information\n",
    "    print(\"Original data info:\")\n",
    "    train_data.info()\n",
    "    \n",
    "    # Encode categorical columns\n",
    "    print(\"\\nPerforming one-hot encoding for categorical variables...\")\n",
    "    categorical_cols = ['service', 'state']\n",
    "    train_data_encoded = one_hot_encode(train_data, categorical_cols)\n",
    "    \n",
    "    # Encode protocol column\n",
    "    print(\"\\nEncoding protocol column...\")\n",
    "    train_data_encoded = encode_protocol(train_data_encoded, column='proto', top_n=10)\n",
    "    \n",
    "    # Display encoded data info\n",
    "    print(\"\\nEncoded data info:\")\n",
    "    train_data_encoded.info()\n",
    "    \n",
    "    # Check protocol columns\n",
    "    proto_columns = [col for col in train_data_encoded.columns if col.startswith('proto_')]\n",
    "    print(f\"Protocol encoded columns: {proto_columns}\")\n",
    "    print(f\"Shape after encoding: {train_data_encoded.shape}\")\n",
    "    \n",
    "    # Check correlation with target (optional)\n",
    "    print(\"\\nChecking correlations with target variable...\")\n",
    "    correlations = train_data_encoded.corr()['label'].sort_values(ascending=False)\n",
    "    print(\"Top positive correlations:\")\n",
    "    print(correlations[correlations > 0].head(10))\n",
    "    print(\"\\nTop negative correlations:\")\n",
    "    print(correlations[correlations < 0].head(10))\n",
    "    \n",
    "    # Visualize top correlations (optional)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = correlations.abs().sort_values(ascending=False).head(20).index\n",
    "    sns.barplot(x=correlations[top_features], y=top_features)\n",
    "    plt.title('Top 20 Feature Correlations with Target')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlations.png')  # Save for later reference\n",
    "    \n",
    "    # Prepare for RBFN\n",
    "    X_cols = [col for col in train_data_encoded.columns if col != 'label']\n",
    "    y_col = 'label'\n",
    "    \n",
    "    # Standardize features\n",
    "    print(\"\\nStandardizing features...\")\n",
    "    train_data_std, scaler_params = standardize_features(train_data_encoded, exclude_cols=[y_col])\n",
    "    \n",
    "    # Split into train and validation\n",
    "    print(\"Splitting data...\")\n",
    "    train_df, val_df = train_validation_split(train_data_std, val_ratio=0.2)\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train = train_df[X_cols].values\n",
    "    y_train = train_df[y_col].values\n",
    "    X_val = val_df[X_cols].values\n",
    "    y_val = val_df[y_col].values\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "    print(f\"Validation set: {X_val.shape[0]} samples, {X_val.shape[1]} features\")\n",
    "    \n",
    "    # Train RBFN model\n",
    "    print(\"\\nTraining RBFN model...\")\n",
    "    start_time = time.time()\n",
    "    # Start with fewer centers to speed up initial testing\n",
    "    rbfn = RBFN(num_centers=140, sigma=7.0)  # Adjust num_centers as needed\n",
    "    rbfn.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(\"\\nEvaluating on validation set...\")\n",
    "    y_val_pred = rbfn.predict(X_val)\n",
    "    val_metrics = evaluate_model(y_val, y_val_pred)\n",
    "    \n",
    "    print(\"\\nValidation Results:\")\n",
    "    print(f\"Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {val_metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {val_metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {val_metrics['f1_score']:.4f}\")\n",
    "    print(f\"Balanced Accuracy: {val_metrics['balanced_accuracy']:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(f\"TP: {val_metrics['confusion_matrix']['tp']}, TN: {val_metrics['confusion_matrix']['tn']}\")\n",
    "    print(f\"FP: {val_metrics['confusion_matrix']['fp']}, FN: {val_metrics['confusion_matrix']['fn']}\")\n",
    "    \n",
    "    # Test on test set 1 if available\n",
    "    try:\n",
    "        print(\"\\nLoading and evaluating on test set 1...\")\n",
    "        test_data1 = pd.read_csv('UNSWNB15_testing1_coursework.csv')\n",
    "        \n",
    "        # Apply the same preprocessing\n",
    "        test_data1_encoded = one_hot_encode(test_data1, categorical_cols)\n",
    "        test_data1_encoded = encode_protocol(test_data1_encoded, column='proto', top_n=10)\n",
    "        \n",
    "        # Handle missing columns (if any)\n",
    "        for col in X_cols:\n",
    "            if col not in test_data1_encoded.columns and col != 'label':\n",
    "                test_data1_encoded[col] = 0  # Add missing columns with zeros\n",
    "        \n",
    "        # Ensure same column order as training data\n",
    "        test_data1_encoded = test_data1_encoded[X_cols + ['label']]\n",
    "        \n",
    "        # Standardize using training parameters\n",
    "        test_data1_std = pd.DataFrame()\n",
    "        for col in X_cols:\n",
    "            if col in scaler_params:\n",
    "                mean = scaler_params[col]['mean']\n",
    "                std = scaler_params[col]['std']\n",
    "                if std > 0:\n",
    "                    test_data1_std[col] = (test_data1_encoded[col] - mean) / std\n",
    "                else:\n",
    "                    test_data1_std[col] = 0\n",
    "            else:\n",
    "                test_data1_std[col] = test_data1_encoded[col]\n",
    "        \n",
    "        test_data1_std['label'] = test_data1_encoded['label']\n",
    "        \n",
    "        # Evaluate\n",
    "        X_test1 = test_data1_std[X_cols].values\n",
    "        y_test1 = test_data1_std['label'].values\n",
    "        y_test1_pred = rbfn.predict(X_test1)\n",
    "        \n",
    "        test1_metrics = evaluate_model(y_test1, y_test1_pred)\n",
    "        \n",
    "        print(\"\\nTest Set 1 Results:\")\n",
    "        print(f\"Accuracy: {test1_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {test1_metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {test1_metrics['recall']:.4f}\")\n",
    "        print(f\"F1 Score: {test1_metrics['f1_score']:.4f}\")\n",
    "        print(f\"Balanced Accuracy: {test1_metrics['balanced_accuracy']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating test set 1: {e}\")\n",
    "    \n",
    "    # Save the model parameters for later use\n",
    "    model_params = {\n",
    "        'centers': rbfn.centers,\n",
    "        'weights': rbfn.weights,\n",
    "        'bias': rbfn.bias,\n",
    "        'sigma': rbfn.sigma,\n",
    "        'scaler_params': scaler_params,\n",
    "        'X_cols': X_cols  # Save column names for consistent preprocessing\n",
    "    }\n",
    "    \n",
    "    return rbfn, model_params, val_metrics\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     rbfn_model, model_params, validation_metrics = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef61d1c0-2f99-4b81-af26-9084d94a1db2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0782b9-5df2-4c49-9f10-cb042b7f420b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1811f891-bc08-4bfd-9c0c-7f1e62e3d537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Encoding functions\n",
    "def one_hot_encode(df, categorical_cols):\n",
    "    \"\"\"\n",
    "    One-hot encode categorical columns\n",
    "    \"\"\"\n",
    "    df_encoded = df.copy()\n",
    "    for col in categorical_cols:\n",
    "        if col in df_encoded.columns:\n",
    "            dummies = pd.get_dummies(df_encoded[col], prefix=col, drop_first=False)\n",
    "            df_encoded = pd.concat([df_encoded, dummies], axis=1)\n",
    "            df_encoded.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "def encode_protocol(df, column='proto', top_n=10):\n",
    "    \"\"\"\n",
    "    Encode protocol column by keeping top N values and grouping others\n",
    "    \"\"\"\n",
    "    if column not in df.columns:\n",
    "        return df\n",
    "        \n",
    "    # Get value counts\n",
    "    value_counts = df[column].value_counts()\n",
    "    \n",
    "    # Identify top N values\n",
    "    top_values = value_counts.head(top_n).index.tolist()\n",
    "    \n",
    "    # Create a copy of the dataframe\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    # Replace all non-top values with 'other'\n",
    "    df_encoded[column] = df_encoded[column].apply(lambda x: x if x in top_values else 'other')\n",
    "    \n",
    "    # One-hot encode the modified column\n",
    "    proto_encoded = pd.get_dummies(df_encoded[column], prefix=column, drop_first=False)\n",
    "    \n",
    "    # Concatenate with original dataframe\n",
    "    df_encoded = pd.concat([df_encoded, proto_encoded], axis=1)\n",
    "    \n",
    "    # Drop the original column\n",
    "    df_encoded.drop(column, axis=1, inplace=True)\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "def perform_cross_validation(X, y, X_cols, rbfn_class, n_splits=5, num_centers=140, sigma=7.0):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation on the data using the provided RBFN class\n",
    "    \"\"\"\n",
    "    # Initialize KFold cross-validator\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Lists to store results for each fold\n",
    "    fold_metrics = []\n",
    "    \n",
    "    print(f\"\\nPerforming {n_splits}-fold cross-validation...\")\n",
    "    \n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        print(f\"\\nTraining on fold {fold+1}/{n_splits}...\")\n",
    "        \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Initialize and train RBFN model with the pre-tuned parameters\n",
    "        rbfn = rbfn_class(num_centers=num_centers, sigma=sigma)\n",
    "        start_time = time.time()\n",
    "        rbfn.fit(X_train, y_train)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Make predictions\n",
    "        y_test_pred = rbfn.predict(X_test)\n",
    "        \n",
    "        # Evaluate\n",
    "        metrics = evaluate_model(y_test, y_test_pred)\n",
    "        metrics['training_time'] = training_time\n",
    "        fold_metrics.append(metrics)\n",
    "        \n",
    "        print(f\"Fold {fold+1} results:\")\n",
    "        print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"  F1 Score: {metrics['f1_score']:.4f}\")\n",
    "        print(f\"  Training time: {metrics['training_time']:.2f} seconds\")\n",
    "    \n",
    "    # Compute average metrics across folds\n",
    "    avg_metrics = {\n",
    "        'accuracy': np.mean([m['accuracy'] for m in fold_metrics]),\n",
    "        'precision': np.mean([m['precision'] for m in fold_metrics]),\n",
    "        'recall': np.mean([m['recall'] for m in fold_metrics]),\n",
    "        'f1_score': np.mean([m['f1_score'] for m in fold_metrics]),\n",
    "        'balanced_accuracy': np.mean([m['balanced_accuracy'] for m in fold_metrics]),\n",
    "        'training_time': np.mean([m['training_time'] for m in fold_metrics])\n",
    "    }\n",
    "    \n",
    "    # Compute standard deviations\n",
    "    std_metrics = {\n",
    "        'accuracy': np.std([m['accuracy'] for m in fold_metrics]),\n",
    "        'precision': np.std([m['precision'] for m in fold_metrics]),\n",
    "        'recall': np.std([m['recall'] for m in fold_metrics]),\n",
    "        'f1_score': np.std([m['f1_score'] for m in fold_metrics]),\n",
    "        'balanced_accuracy': np.std([m['balanced_accuracy'] for m in fold_metrics])\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'fold_metrics': fold_metrics,\n",
    "        'avg_metrics': avg_metrics,\n",
    "        'std_metrics': std_metrics\n",
    "    }\n",
    "\n",
    "# 5. Main execution\n",
    "def main():\n",
    "    # Load training data\n",
    "    print(\"Loading training data...\")\n",
    "    train_data = pd.read_csv('UNSWNB15_training_coursework.csv')\n",
    "    \n",
    "    # Display basic information\n",
    "    print(\"Original data info:\")\n",
    "    train_data.info()\n",
    "    \n",
    "    # Encode categorical columns\n",
    "    print(\"\\nPerforming one-hot encoding for categorical variables...\")\n",
    "    categorical_cols = ['service', 'state']\n",
    "    train_data_encoded = one_hot_encode(train_data, categorical_cols)\n",
    "    \n",
    "    # Encode protocol column\n",
    "    print(\"\\nEncoding protocol column...\")\n",
    "    train_data_encoded = encode_protocol(train_data_encoded, column='proto', top_n=10)\n",
    "    \n",
    "    # Display encoded data info\n",
    "    print(\"\\nEncoded data info:\")\n",
    "    train_data_encoded.info()\n",
    "    \n",
    "    # Check protocol columns\n",
    "    proto_columns = [col for col in train_data_encoded.columns if col.startswith('proto_')]\n",
    "    print(f\"Protocol encoded columns: {proto_columns}\")\n",
    "    print(f\"Shape after encoding: {train_data_encoded.shape}\")\n",
    "    \n",
    "    # Check correlation with target (optional)\n",
    "    print(\"\\nChecking correlations with target variable...\")\n",
    "    correlations = train_data_encoded.corr()['label'].sort_values(ascending=False)\n",
    "    print(\"Top positive correlations:\")\n",
    "    print(correlations[correlations > 0].head(10))\n",
    "    print(\"\\nTop negative correlations:\")\n",
    "    print(correlations[correlations < 0].head(10))\n",
    "    \n",
    "    # Visualize top correlations (optional)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = correlations.abs().sort_values(ascending=False).head(20).index\n",
    "    sns.barplot(x=correlations[top_features], y=top_features)\n",
    "    plt.title('Top 20 Feature Correlations with Target')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlations.png')  # Save for later reference\n",
    "    \n",
    "    # Prepare for RBFN\n",
    "    X_cols = [col for col in train_data_encoded.columns if col != 'label']\n",
    "    y_col = 'label'\n",
    "    \n",
    "    # Standardize features\n",
    "    print(\"\\nStandardizing features...\")\n",
    "    train_data_std, scaler_params = standardize_features(train_data_encoded, exclude_cols=[y_col])\n",
    "    \n",
    "    # Extract X and y for cross-validation\n",
    "    X = train_data_std[X_cols].values\n",
    "    y = train_data_std[y_col].values\n",
    "    \n",
    "    # Perform cross-validation using your RBFN model with pre-tuned parameters\n",
    "    print(\"\\nPerforming cross-validation with pre-tuned parameters (centers=140, sigma=7.0)...\")\n",
    "    cv_results = perform_cross_validation(X, y, X_cols, RBFN, n_splits=5, num_centers=140, sigma=7.0)\n",
    "    \n",
    "    # Print cross-validation results\n",
    "    print(\"\\nCross-Validation Results Summary:\")\n",
    "    print(f\"Average Accuracy: {cv_results['avg_metrics']['accuracy']:.4f} ± {cv_results['std_metrics']['accuracy']:.4f}\")\n",
    "    print(f\"Average Precision: {cv_results['avg_metrics']['precision']:.4f} ± {cv_results['std_metrics']['precision']:.4f}\")\n",
    "    print(f\"Average Recall: {cv_results['avg_metrics']['recall']:.4f} ± {cv_results['std_metrics']['recall']:.4f}\")\n",
    "    print(f\"Average F1 Score: {cv_results['avg_metrics']['f1_score']:.4f} ± {cv_results['std_metrics']['f1_score']:.4f}\")\n",
    "    print(f\"Average Balanced Accuracy: {cv_results['avg_metrics']['balanced_accuracy']:.4f} ± {cv_results['std_metrics']['balanced_accuracy']:.4f}\")\n",
    "    print(f\"Average Training Time: {cv_results['avg_metrics']['training_time']:.2f} seconds\")\n",
    "    \n",
    "    # Visualize cross-validation results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'balanced_accuracy']\n",
    "    values = [cv_results['avg_metrics'][m] for m in metrics]\n",
    "    errors = [cv_results['std_metrics'][m] for m in metrics]\n",
    "    \n",
    "    plt.bar(metrics, values, yerr=errors, capsize=10)\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.title('Cross-Validation Performance Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig('cv_results.png')\n",
    "    \n",
    "    # Split into train and validation\n",
    "    print(\"Splitting data...\")\n",
    "    train_df, val_df = train_validation_split(train_data_std, val_ratio=0.2)\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train = train_df[X_cols].values\n",
    "    y_train = train_df[y_col].values\n",
    "    X_val = val_df[X_cols].values\n",
    "    y_val = val_df[y_col].values\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "    print(f\"Validation set: {X_val.shape[0]} samples, {X_val.shape[1]} features\")\n",
    "    \n",
    "    # Train RBFN model with pre-tuned parameters\n",
    "    print(\"\\nTraining RBFN model...\")\n",
    "    start_time = time.time()\n",
    "    rbfn = RBFN(num_centers=140, sigma=7.0)  # Use pre-tuned parameters\n",
    "    rbfn.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(\"\\nEvaluating on validation set...\")\n",
    "    y_val_pred = rbfn.predict(X_val)\n",
    "    val_metrics = evaluate_model(y_val, y_val_pred)\n",
    "    \n",
    "    print(\"\\nValidation Results:\")\n",
    "    print(f\"Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {val_metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {val_metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {val_metrics['f1_score']:.4f}\")\n",
    "    print(f\"Balanced Accuracy: {val_metrics['balanced_accuracy']:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(f\"TP: {val_metrics['confusion_matrix']['tp']}, TN: {val_metrics['confusion_matrix']['tn']}\")\n",
    "    print(f\"FP: {val_metrics['confusion_matrix']['fp']}, FN: {val_metrics['confusion_matrix']['fn']}\")\n",
    "    \n",
    "    # Test on test set 1 if available\n",
    "    try:\n",
    "        print(\"\\nLoading and evaluating on test set 1...\")\n",
    "        test_data1 = pd.read_csv('UNSWNB15_testing1_coursework.csv')\n",
    "        \n",
    "        # Apply the same preprocessing\n",
    "        test_data1_encoded = one_hot_encode(test_data1, categorical_cols)\n",
    "        test_data1_encoded = encode_protocol(test_data1_encoded, column='proto', top_n=10)\n",
    "        \n",
    "        # Handle missing columns (if any)\n",
    "        for col in X_cols:\n",
    "            if col not in test_data1_encoded.columns and col != 'label':\n",
    "                test_data1_encoded[col] = 0  # Add missing columns with zeros\n",
    "        \n",
    "        # Ensure same column order as training data\n",
    "        test_data1_encoded = test_data1_encoded[X_cols + ['label']]\n",
    "        \n",
    "        # Standardize using training parameters\n",
    "        test_data1_std = pd.DataFrame()\n",
    "        for col in X_cols:\n",
    "            if col in scaler_params:\n",
    "                mean = scaler_params[col]['mean']\n",
    "                std = scaler_params[col]['std']\n",
    "                if std > 0:\n",
    "                    test_data1_std[col] = (test_data1_encoded[col] - mean) / std\n",
    "                else:\n",
    "                    test_data1_std[col] = 0\n",
    "            else:\n",
    "                test_data1_std[col] = test_data1_encoded[col]\n",
    "        \n",
    "        test_data1_std['label'] = test_data1_encoded['label']\n",
    "        \n",
    "        # Evaluate\n",
    "        X_test1 = test_data1_std[X_cols].values\n",
    "        y_test1 = test_data1_std['label'].values\n",
    "        y_test1_pred = rbfn.predict(X_test1)\n",
    "        \n",
    "        test1_metrics = evaluate_model(y_test1, y_test1_pred)\n",
    "        \n",
    "        print(\"\\nTest Set 1 Results:\")\n",
    "        print(f\"Accuracy: {test1_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {test1_metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {test1_metrics['recall']:.4f}\")\n",
    "        print(f\"F1 Score: {test1_metrics['f1_score']:.4f}\")\n",
    "        print(f\"Balanced Accuracy: {test1_metrics['balanced_accuracy']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating test set 1: {e}\")\n",
    "    \n",
    "    # Save the model parameters for later use\n",
    "    model_params = {\n",
    "        'centers': rbfn.centers,\n",
    "        'weights': rbfn.weights,\n",
    "        'bias': rbfn.bias,\n",
    "        'sigma': rbfn.sigma,\n",
    "        'scaler_params': scaler_params,\n",
    "        'X_cols': X_cols,  # Save column names for consistent preprocessing\n",
    "        'cv_results': cv_results  # Include cross-validation results\n",
    "    }\n",
    "    \n",
    "    return rbfn, model_params, val_metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f1d2db-0a5f-4354-aced-508076a3e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     rbfn_model, model_params, validation_metrics = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596c5ec1-4208-40b7-86d4-d81fb2410531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaeefaf-5708-4a91-9cae-c5fe0fc32d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Encoding functions\n",
    "def one_hot_encode(df, categorical_cols):\n",
    "    \"\"\"\n",
    "    One-hot encode categorical columns\n",
    "    \"\"\"\n",
    "    df_encoded = df.copy()\n",
    "    for col in categorical_cols:\n",
    "        if col in df_encoded.columns:\n",
    "            dummies = pd.get_dummies(df_encoded[col], prefix=col, drop_first=False)\n",
    "            df_encoded = pd.concat([df_encoded, dummies], axis=1)\n",
    "            df_encoded.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "def encode_protocol(df, column='proto', top_n=10):\n",
    "    \"\"\"\n",
    "    Encode protocol column by keeping top N values and grouping others\n",
    "    \"\"\"\n",
    "    if column not in df.columns:\n",
    "        return df\n",
    "        \n",
    "    # Get value counts\n",
    "    value_counts = df[column].value_counts()\n",
    "    \n",
    "    # Identify top N values\n",
    "    top_values = value_counts.head(top_n).index.tolist()\n",
    "    \n",
    "    # Create a copy of the dataframe\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    # Replace all non-top values with 'other'\n",
    "    df_encoded[column] = df_encoded[column].apply(lambda x: x if x in top_values else 'other')\n",
    "    \n",
    "    # One-hot encode the modified column\n",
    "    proto_encoded = pd.get_dummies(df_encoded[column], prefix=column, drop_first=False)\n",
    "    \n",
    "    # Concatenate with original dataframe\n",
    "    df_encoded = pd.concat([df_encoded, proto_encoded], axis=1)\n",
    "    \n",
    "    # Drop the original column\n",
    "    df_encoded.drop(column, axis=1, inplace=True)\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "# Add a class for RBFN with probability support\n",
    "class RBFN:\n",
    "    def __init__(self, num_centers=100, sigma=1.0):\n",
    "        self.num_centers = num_centers\n",
    "        self.sigma = sigma\n",
    "        self.centers = None\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def _rbf_kernel(self, x, center):\n",
    "        \"\"\"Compute RBF kernel\"\"\"\n",
    "        return np.exp(-np.linalg.norm(x - center)**2 / (2 * self.sigma**2))\n",
    "    \n",
    "    def _compute_activations(self, X):\n",
    "        \"\"\"Compute activations for each center for all samples in X\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        activations = np.zeros((n_samples, self.num_centers))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            for j in range(self.num_centers):\n",
    "                activations[i, j] = self._rbf_kernel(X[i], self.centers[j])\n",
    "                \n",
    "        return activations\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit RBFN model using k-means for centers and least squares for weights\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Select centers using k-means or random selection\n",
    "        if self.num_centers < n_samples:\n",
    "            # Use k-means\n",
    "            from sklearn.cluster import KMeans\n",
    "            kmeans = KMeans(n_clusters=self.num_centers, random_state=42)\n",
    "            kmeans.fit(X)\n",
    "            self.centers = kmeans.cluster_centers_\n",
    "        else:\n",
    "            # Use all training examples as centers\n",
    "            self.num_centers = n_samples\n",
    "            self.centers = X.copy()\n",
    "        \n",
    "        # Compute activations using RBF kernel\n",
    "        activations = self._compute_activations(X)\n",
    "        \n",
    "        # Add bias term\n",
    "        activations_bias = np.column_stack((activations, np.ones(n_samples)))\n",
    "        \n",
    "        # Calculate weights using least squares (pseudoinverse)\n",
    "        pseudoinv = np.linalg.pinv(activations_bias)\n",
    "        weights_bias = np.dot(pseudoinv, y)\n",
    "        \n",
    "        # Separate weights and bias\n",
    "        self.weights = weights_bias[:-1]\n",
    "        self.bias = weights_bias[-1]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict probabilities using the trained model\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Compute activations for test samples\n",
    "        activations = self._compute_activations(X)\n",
    "        \n",
    "        # Compute raw output (dot product with weights and add bias)\n",
    "        raw_output = np.dot(activations, self.weights) + self.bias\n",
    "        \n",
    "        # Apply sigmoid function to get probabilities\n",
    "        probabilities = 1 / (1 + np.exp(-raw_output))\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Predict class labels using the trained model with threshold\"\"\"\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return (probabilities >= threshold).astype(int)\n",
    "\n",
    "def standardize_features(df, exclude_cols=None):\n",
    "    \"\"\"\n",
    "    Standardize numerical features in dataframe\n",
    "    \"\"\"\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = []\n",
    "        \n",
    "    df_std = df.copy()\n",
    "    scaler_params = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col not in exclude_cols:\n",
    "            # Calculate mean and std\n",
    "            mean = df[col].mean()\n",
    "            std = df[col].std()\n",
    "            \n",
    "            # Store parameters for future use\n",
    "            scaler_params[col] = {'mean': mean, 'std': std}\n",
    "            \n",
    "            # Apply standardization\n",
    "            if std > 0:\n",
    "                df_std[col] = (df[col] - mean) / std\n",
    "            # If std is 0, leave the column as is\n",
    "    \n",
    "    return df_std, scaler_params\n",
    "\n",
    "def train_validation_split(df, val_ratio=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split dataframe into training and validation sets\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Get indices of samples\n",
    "    indices = np.random.permutation(df.shape[0])\n",
    "    val_size = int(val_ratio * df.shape[0])\n",
    "    \n",
    "    # Split indices\n",
    "    val_indices = indices[:val_size]\n",
    "    train_indices = indices[val_size:]\n",
    "    \n",
    "    # Create dataframes\n",
    "    train_df = df.iloc[train_indices].copy()\n",
    "    val_df = df.iloc[val_indices].copy()\n",
    "    \n",
    "    return train_df, val_df\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluate model performance using various metrics\n",
    "    \"\"\"\n",
    "    # Calculate confusion matrix components\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Calculate balanced accuracy\n",
    "    sensitivity = recall\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    balanced_accuracy = (sensitivity + specificity) / 2\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "        'balanced_accuracy': balanced_accuracy,\n",
    "        'confusion_matrix': {\n",
    "            'tp': int(tp),\n",
    "            'tn': int(tn),\n",
    "            'fp': int(fp),\n",
    "            'fn': int(fn)\n",
    "        }\n",
    "    }\n",
    "\n",
    "def perform_cross_validation(X, y, X_cols, rbfn_class, n_splits=5, num_centers=140, sigma=7.0, threshold=0.6):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation on the data using the provided RBFN class\n",
    "    \"\"\"\n",
    "    # Initialize KFold cross-validator\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Lists to store results for each fold\n",
    "    fold_metrics = []\n",
    "    \n",
    "    print(f\"\\nPerforming {n_splits}-fold cross-validation...\")\n",
    "    \n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        print(f\"\\nTraining on fold {fold+1}/{n_splits}...\")\n",
    "        \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Initialize and train RBFN model with the pre-tuned parameters\n",
    "        rbfn = rbfn_class(num_centers=num_centers, sigma=sigma)\n",
    "        start_time = time.time()\n",
    "        rbfn.fit(X_train, y_train)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Make predictions with threshold\n",
    "        y_test_pred = rbfn.predict(X_test, threshold=threshold)\n",
    "        \n",
    "        # Evaluate\n",
    "        metrics = evaluate_model(y_test, y_test_pred)\n",
    "        metrics['training_time'] = training_time\n",
    "        fold_metrics.append(metrics)\n",
    "        \n",
    "        print(f\"Fold {fold+1} results (threshold={threshold}):\")\n",
    "        print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"  F1 Score: {metrics['f1_score']:.4f}\")\n",
    "        print(f\"  Training time: {metrics['training_time']:.2f} seconds\")\n",
    "    \n",
    "    # Compute average metrics across folds\n",
    "    avg_metrics = {\n",
    "        'accuracy': np.mean([m['accuracy'] for m in fold_metrics]),\n",
    "        'precision': np.mean([m['precision'] for m in fold_metrics]),\n",
    "        'recall': np.mean([m['recall'] for m in fold_metrics]),\n",
    "        'f1_score': np.mean([m['f1_score'] for m in fold_metrics]),\n",
    "        'balanced_accuracy': np.mean([m['balanced_accuracy'] for m in fold_metrics]),\n",
    "        'training_time': np.mean([m['training_time'] for m in fold_metrics])\n",
    "    }\n",
    "    \n",
    "    # Compute standard deviations\n",
    "    std_metrics = {\n",
    "        'accuracy': np.std([m['accuracy'] for m in fold_metrics]),\n",
    "        'precision': np.std([m['precision'] for m in fold_metrics]),\n",
    "        'recall': np.std([m['recall'] for m in fold_metrics]),\n",
    "        'f1_score': np.std([m['f1_score'] for m in fold_metrics]),\n",
    "        'balanced_accuracy': np.std([m['balanced_accuracy'] for m in fold_metrics])\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'fold_metrics': fold_metrics,\n",
    "        'avg_metrics': avg_metrics,\n",
    "        'std_metrics': std_metrics\n",
    "    }\n",
    "\n",
    "# 5. Main execution\n",
    "def main():\n",
    "    # Threshold for prediction\n",
    "    threshold = 0.6\n",
    "    \n",
    "    # Load training data\n",
    "    print(\"Loading training data...\")\n",
    "    train_data = pd.read_csv('UNSWNB15_training_coursework.csv')\n",
    "    \n",
    "    # Display basic information\n",
    "    print(\"Original data info:\")\n",
    "    train_data.info()\n",
    "    \n",
    "    # Encode categorical columns\n",
    "    print(\"\\nPerforming one-hot encoding for categorical variables...\")\n",
    "    categorical_cols = ['service', 'state']\n",
    "    train_data_encoded = one_hot_encode(train_data, categorical_cols)\n",
    "    \n",
    "    # Encode protocol column\n",
    "    print(\"\\nEncoding protocol column...\")\n",
    "    train_data_encoded = encode_protocol(train_data_encoded, column='proto', top_n=10)\n",
    "    \n",
    "    # Display encoded data info\n",
    "    print(\"\\nEncoded data info:\")\n",
    "    train_data_encoded.info()\n",
    "    \n",
    "    # Check protocol columns\n",
    "    proto_columns = [col for col in train_data_encoded.columns if col.startswith('proto_')]\n",
    "    print(f\"Protocol encoded columns: {proto_columns}\")\n",
    "    print(f\"Shape after encoding: {train_data_encoded.shape}\")\n",
    "    \n",
    "    # Check correlation with target (optional)\n",
    "    print(\"\\nChecking correlations with target variable...\")\n",
    "    correlations = train_data_encoded.corr()['label'].sort_values(ascending=False)\n",
    "    print(\"Top positive correlations:\")\n",
    "    print(correlations[correlations > 0].head(10))\n",
    "    print(\"\\nTop negative correlations:\")\n",
    "    print(correlations[correlations < 0].head(10))\n",
    "    \n",
    "    # Visualize top correlations (optional)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = correlations.abs().sort_values(ascending=False).head(20).index\n",
    "    sns.barplot(x=correlations[top_features], y=top_features)\n",
    "    plt.title('Top 20 Feature Correlations with Target')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlations.png')  # Save for later reference\n",
    "    \n",
    "    # Prepare for RBFN\n",
    "    X_cols = [col for col in train_data_encoded.columns if col != 'label']\n",
    "    y_col = 'label'\n",
    "    \n",
    "    # Standardize features\n",
    "    print(\"\\nStandardizing features...\")\n",
    "    train_data_std, scaler_params = standardize_features(train_data_encoded, exclude_cols=[y_col])\n",
    "    \n",
    "    # Extract X and y for cross-validation\n",
    "    X = train_data_std[X_cols].values\n",
    "    y = train_data_std[y_col].values\n",
    "    \n",
    "    # Perform cross-validation using RBFN model with pre-tuned parameters and threshold\n",
    "    print(f\"\\nPerforming cross-validation with pre-tuned parameters (centers=140, sigma=7.0, threshold={threshold})...\")\n",
    "    cv_results = perform_cross_validation(X, y, X_cols, RBFN, n_splits=5, num_centers=140, sigma=7.0, threshold=threshold)\n",
    "    \n",
    "    # Print cross-validation results\n",
    "    print(\"\\nCross-Validation Results Summary (with threshold):\")\n",
    "    print(f\"Threshold: {threshold}\")\n",
    "    print(f\"Average Accuracy: {cv_results['avg_metrics']['accuracy']:.4f} ± {cv_results['std_metrics']['accuracy']:.4f}\")\n",
    "    print(f\"Average Precision: {cv_results['avg_metrics']['precision']:.4f} ± {cv_results['std_metrics']['precision']:.4f}\")\n",
    "    print(f\"Average Recall: {cv_results['avg_metrics']['recall']:.4f} ± {cv_results['std_metrics']['recall']:.4f}\")\n",
    "    print(f\"Average F1 Score: {cv_results['avg_metrics']['f1_score']:.4f} ± {cv_results['std_metrics']['f1_score']:.4f}\")\n",
    "    print(f\"Average Balanced Accuracy: {cv_results['avg_metrics']['balanced_accuracy']:.4f} ± {cv_results['std_metrics']['balanced_accuracy']:.4f}\")\n",
    "    print(f\"Average Training Time: {cv_results['avg_metrics']['training_time']:.2f} seconds\")\n",
    "    \n",
    "    # Visualize cross-validation results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'balanced_accuracy']\n",
    "    values = [cv_results['avg_metrics'][m] for m in metrics]\n",
    "    errors = [cv_results['std_metrics'][m] for m in metrics]\n",
    "    \n",
    "    plt.bar(metrics, values, yerr=errors, capsize=10)\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.title(f'Cross-Validation Performance Metrics (Threshold = {threshold})')\n",
    "    plt.ylabel('Score')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig('cv_results_threshold.png')\n",
    "    \n",
    "    # Split into train and validation\n",
    "    print(\"Splitting data...\")\n",
    "    train_df, val_df = train_validation_split(train_data_std, val_ratio=0.2)\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train = train_df[X_cols].values\n",
    "    y_train = train_df[y_col].values\n",
    "    X_val = val_df[X_cols].values\n",
    "    y_val = val_df[y_col].values\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "    print(f\"Validation set: {X_val.shape[0]} samples, {X_val.shape[1]} features\")\n",
    "    \n",
    "    # Train RBFN model with pre-tuned parameters\n",
    "    print(\"\\nTraining RBFN model...\")\n",
    "    start_time = time.time()\n",
    "    rbfn = RBFN(num_centers=140, sigma=7.0)  # Use pre-tuned parameters\n",
    "    rbfn.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Get probability predictions\n",
    "    print(f\"\\nEvaluating on validation set with threshold {threshold}...\")\n",
    "    val_proba = rbfn.predict_proba(X_val)\n",
    "    y_val_pred = (val_proba >= threshold).astype(int)\n",
    "    val_metrics = evaluate_model(y_val, y_val_pred)\n",
    "    \n",
    "    # Visualize probability distributions\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(val_proba[y_val == 0], bins=50, alpha=0.5, label='Negative class (0)')\n",
    "    plt.hist(val_proba[y_val == 1], bins=50, alpha=0.5, label='Positive class (1)')\n",
    "    plt.axvline(x=threshold, color='red', linestyle='--', label=f'Threshold = {threshold}')\n",
    "    plt.title('Probability Distribution by Class')\n",
    "    plt.xlabel('Probability')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('probability_distribution.png')\n",
    "    \n",
    "    print(\"\\nValidation Results:\")\n",
    "    print(f\"Threshold: {threshold}\")\n",
    "    print(f\"Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {val_metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {val_metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {val_metrics['f1_score']:.4f}\")\n",
    "    print(f\"Balanced Accuracy: {val_metrics['balanced_accuracy']:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(f\"TP: {val_metrics['confusion_matrix']['tp']}, TN: {val_metrics['confusion_matrix']['tn']}\")\n",
    "    print(f\"FP: {val_metrics['confusion_matrix']['fp']}, FN: {val_metrics['confusion_matrix']['fn']}\")\n",
    "    \n",
    "    # Test on test set 1 if available\n",
    "    try:\n",
    "        print(\"\\nLoading and evaluating on test set 1...\")\n",
    "        test_data1 = pd.read_csv('UNSWNB15_testing1_coursework.csv')\n",
    "        \n",
    "        # Apply the same preprocessing\n",
    "        test_data1_encoded = one_hot_encode(test_data1, categorical_cols)\n",
    "        test_data1_encoded = encode_protocol(test_data1_encoded, column='proto', top_n=10)\n",
    "        \n",
    "        # Handle missing columns (if any)\n",
    "        for col in X_cols:\n",
    "            if col not in test_data1_encoded.columns and col != 'label':\n",
    "                test_data1_encoded[col] = 0  # Add missing columns with zeros\n",
    "        \n",
    "        # Ensure same column order as training data\n",
    "        test_data1_encoded = test_data1_encoded[X_cols + ['label']]\n",
    "        \n",
    "        # Standardize using training parameters\n",
    "        test_data1_std = pd.DataFrame()\n",
    "        for col in X_cols:\n",
    "            if col in scaler_params:\n",
    "                mean = scaler_params[col]['mean']\n",
    "                std = scaler_params[col]['std']\n",
    "                if std > 0:\n",
    "                    test_data1_std[col] = (test_data1_encoded[col] - mean) / std\n",
    "                else:\n",
    "                    test_data1_std[col] = 0\n",
    "            else:\n",
    "                test_data1_std[col] = test_data1_encoded[col]\n",
    "        \n",
    "        test_data1_std['label'] = test_data1_encoded['label']\n",
    "        \n",
    "        # Evaluate with threshold\n",
    "        X_test1 = test_data1_std[X_cols].values\n",
    "        y_test1 = test_data1_std['label'].values\n",
    "        \n",
    "        # Get probability predictions and apply threshold\n",
    "        test_proba = rbfn.predict_proba(X_test1)\n",
    "        y_test1_pred = (test_proba >= threshold).astype(int)\n",
    "        \n",
    "        test1_metrics = evaluate_model(y_test1, y_test1_pred)\n",
    "        \n",
    "        print(\"\\nTest Set 1 Results:\")\n",
    "        print(f\"Threshold: {threshold}\")\n",
    "        print(f\"Accuracy: {test1_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {test1_metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {test1_metrics['recall']:.4f}\")\n",
    "        print(f\"F1 Score: {test1_metrics['f1_score']:.4f}\")\n",
    "        print(f\"Balanced Accuracy: {test1_metrics['balanced_accuracy']:.4f}\")\n",
    "        \n",
    "        # Visualize probability distributions for test set\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(test_proba[y_test1 == 0], bins=50, alpha=0.5, label='Negative class (0)')\n",
    "        plt.hist(test_proba[y_test1 == 1], bins=50, alpha=0.5, label='Positive class (1)')\n",
    "        plt.axvline(x=threshold, color='red', linestyle='--', label=f'Threshold = {threshold}')\n",
    "        plt.title('Test Set Probability Distribution by Class')\n",
    "        plt.xlabel('Probability')\n",
    "        plt.ylabel('Count')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig('test_probability_distribution.png')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating test set 1: {e}\")\n",
    "    \n",
    "    # Save the model parameters for later use\n",
    "    model_params = {\n",
    "        'centers': rbfn.centers,\n",
    "        'weights': rbfn.weights,\n",
    "        'bias': rbfn.bias,\n",
    "        'sigma': rbfn.sigma,\n",
    "        'threshold': threshold,  # Save the threshold\n",
    "        'scaler_params': scaler_params,\n",
    "        'X_cols': X_cols,  # Save column names for consistent preprocessing\n",
    "        'cv_results': cv_results  # Include cross-validation results\n",
    "    }\n",
    "    \n",
    "    return rbfn, model_params, val_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    rbfn_model, model_params, validation_metrics = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4291283-b12c-4068-80ae-546d46844c14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
